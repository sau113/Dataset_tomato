{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Untitled1.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sau113/Dataset_tomato/blob/master/Trainer2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "QP_QrGqaNdzg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 194
        },
        "outputId": "c0b6beb7-e38b-4c14-de30-0681209a72ad"
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "import glob\n",
        "import numpy as np\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow import keras \n",
        "import tensorflow as tf\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Convolution2D\n",
        "from keras.layers import MaxPooling2D\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Dense\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "from keras.utils import np_utils\n",
        "\n",
        "network = Sequential()\n",
        "\n",
        "network.add(Convolution2D(32,3,3,input_shape = (64,64,3),activation = 'relu'))\n",
        "\n",
        "network.add(MaxPooling2D(pool_size = (2,2)))\n",
        "\n",
        "network.add(Convolution2D(32, 3, 3, activation = 'relu'))\n",
        "network.add(MaxPooling2D(pool_size = (2, 2)))\n",
        "\n",
        "network.add(Convolution2D(32, 3, 3, activation = 'relu'))\n",
        "network.add(MaxPooling2D(pool_size = (2, 2)))\n",
        "\n",
        "network.add(Flatten())\n",
        "\n",
        "network.add(Dense(output_dim = 128, activation = 'relu'))\n",
        "network.add(Dense(output_dim = 64, activation = 'relu'))\n",
        "network.add(Dense(output_dim = 6, activation = 'sigmoid'))\n",
        "\n",
        "network.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
        "\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "train_data = ImageDataGenerator(rescale = 1./255, \n",
        "                                shear_range = 0.2,\n",
        "                                zoom_range = 0.2,\n",
        "                                horizontal_flip = True)\n",
        "\n",
        "valid_data = ImageDataGenerator(rescale = 1./255)\n",
        "\n",
        "\n",
        "\n",
        "! git clone https://github.com/sau113/Dataset_tomato.git\n",
        "\n",
        "\n",
        "  \n",
        "training_set = train_data.flow_from_directory('Dataset_tomato/Training_set',\n",
        "                                              target_size = (64,64),\n",
        "                                              batch_size = 10,\n",
        "                                              class_mode = \"categorical\")\n",
        "\n",
        "imgs, labels = next(training_set)\n",
        "\n",
        "print(labels[:,:])\n",
        "\n",
        "valid_set= valid_data.flow_from_directory('Dataset_tomato/Validation_set',\n",
        "                                          target_size = (64,64),\n",
        "                                          batch_size=10,\n",
        "                                          class_mode=\"categorical\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "network.fit_generator(training_set,\n",
        "                      steps_per_epoch = (5520+800)/10,\n",
        "                      epochs = 5 ,\n",
        "                      validation_data = valid_set,\n",
        "                      validation_steps = (4920+200)/10)\n",
        "\n",
        "\n",
        "% cd Dataset_tomato\n",
        "network.save('train_data.h5')\n",
        "\n",
        "! ls"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0. 0. 0. 1. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 1.]\n",
            " [1. 0. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 1. 0. 0.]\n",
            " [0. 0. 0. 1. 0. 0.]\n",
            " [0. 0. 0. 1. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 1.]\n",
            " [0. 0. 1. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 1.]]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}